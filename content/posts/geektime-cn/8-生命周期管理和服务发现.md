---
title: "模块八：生命周期管理和服务发现"
date: 2022-1107T15:15:25+08:00
tags: ["geektime-cn"]
---



## 1.深入理解pod的生命周期



## 2.服务发现



## 3.Service对象

Service类型:
- ClusterIP
- NodePort
- LoadBalancer
- Headless
- ExternalName


Service的拓扑支持

## 4.kube-proxy组件


iptables相关命令
```bash
iptables -L -d nat #查看nat类型的iptables规则
iptables-save -t nat # 查看在本机执行过的iptable命令
```


iptable切换ipvs的操作


企业外部访问k8s内部服务方案：
- 创建dns记录指向内部loadbalancer的vip


## 5.DNS原理和实践

## 6.Ingress对象


## 7.案例分享


## 课后作业

### 第一部分

作业要求：编写 Kubernetes 部署脚本将 httpserver 部署到 Kubernetes 集群，以下是你可以思考的维度。

> 完整yaml脚本点击[链接](https://github.com/Giuliao/cn-practices/blob/main/module2/.build/build-stage.yml)
> 完整http server代码点击[链接](https://github.com/Giuliao/cn-practices/blob/main/module2/main.go)

__1. 优雅启动__

主要体现在存活探针，可参见4探活部分

__2. 优雅终止__

主要分为两部分，容器脚本处理，httpserver代码逻辑处理。

在部署的deployment里面设置preStop lifecycle. 主要为shell脚本循环调用killall 确保进程已被kill。killall比起kill的优势在于，应用被kill返回0未被kill返回1.
```yaml
 lifecycle:
    preStop:
        exec:
            command: [ "/bin/sh","-c","while killall /bin/server; do sleep 1; done" ]
```


应用的优雅关闭代码如下，具体解释请参考注释：
```go
func gracefullyExit(server *http.Server) {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	glog.Info("server will shutdown in 5 seconds")
	// https://stackoverflow.com/questions/39320025/how-to-stop-http-listenandserve
	if err := server.Shutdown(ctx); err != nil {
		glog.Fatal(err)
	}
}

// https://cloud.tencent.com/developer/article/1645996
func dealSysSignal(server *http.Server, wg *sync.WaitGroup, c chan os.Signal) {

// 一旦接受到中断信号就跳出循环，防止脚本循环发kill消息，反复执行gracefullExit函数调用
Loop:
	for s := range c {
		switch s {
		case syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT:
			break Loop
		}
	}
    // 优雅关闭主逻辑
	gracefullyExit(server)
	wg.Done()
}

func newServerHandler() *http.ServeMux {
	interceptor := func(hf http.HandlerFunc) http.HandlerFunc {
		return middleware.SetVersion(middleware.GetIP(middleware.GetResponseStatus(hf)))
	}
	mux := http.NewServeMux()
	mux.HandleFunc("/", interceptor(handler.Index))
	mux.HandleFunc("/healthz", interceptor(handler.Healthz))
	return mux
}

func main() {
	flag.Parse()

	wg := &sync.WaitGroup{}
	server := http.Server{Addr: fmt.Sprintf(":%d", portFlag), Handler: newServerHandler()}

    // 1. 设置关闭server的回调函数，通过waitgroup确保调用完成
	wg.Add(1)
	server.RegisterOnShutdown(func() {
		wg.Done()
		glog.Info("server is shutdown")
	})

    // 2. 设置中断信号监听
	c := make(chan os.Signal)
	signal.Notify(c, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM)
	wg.Add(1)

    // 3. 处理中断信号goroutine
	go dealSysSignal(&server, wg, c)

	err := server.ListenAndServe()
	if err != nil && err != http.ErrServerClosed {
		glog.Fatal(err)
	}

    // 4. 等待函数处理完成功返回
	wg.Wait()

    // 5. 关闭中断信号监听，关闭channel
	// avoid send close channel
	signal.Stop(c)
	close(c)
	os.Exit(0)
}
```

测试说明：deployment设置rs为0，再观察pod日志

```bash
kubectl edit deploy -n wg-ns simple-server # 设置rs为0
kubectl logs -f -n wg-ns simple-server-8476997874-n88gv # 监听日志
```

查看pod的logs具体日志信息如下：
```bash
I1120 07:37:53.908754       1 interceptor.go:65] request /healthz status code 200
I1120 07:37:53.908761       1 interceptor.go:78] request /healthz ip addr 172.21.0.114
I1120 07:38:00.056977       1 main.go:33] server will shutdown in 5 seconds
I1120 07:38:00.057048       1 main.go:72] server is shutdown
```

__3. 资源需求和 QoS 保证__

这部分应该给应用做一个压测，评估初步的内存和cpu使用。当前使用浏览器不停服务路径，并根据top命令做了个初步的测试

```bash
top -pid <processid>
```
>连续不停访问cpu会轻微上涨，停止访问后就下降。内存短时间内飙升，过个几分钟下降到3M左右

|  %CPU  |  MEM |
|  ----  | ----  |
|  0.1%  | 2944K |

```yaml
 resources:
    limits:
    cpu: 1000m
    memory: 1024Mi
    requests:
    cpu: 700m
    memory: 200Mi
```

__4. 探活__

探活yaml信息如下：
```yaml
 livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
```

__5. 日常运维需求，日志等级__

日志等级体现在应用启动参数,这部分在dockerfile内就固定了，但是可通过启动命令覆盖CMD内的启动参数。
```dockerfile
ENTRYPOINT ["/bin/server"] 
CMD ["-logtostderr=true", "-port=8080"]
```

__6. 配置和代码分离__

配置主要体现在访问server时会在response添加version信息，该部分使用configmap生成配置注入到pod中
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-server-env-config
  namespace: wg-ns
data:
  version: v2
```


### 第二部分

来尝试用 Service, Ingress 将你的服务发布给集群外部的调用方吧。
在第一部分的基础上提供更加完备的部署 spec，包括（不限于）：

__1. Service__


服务的yaml如下：
```yaml
apiVersion: v1
kind: Service
metadata:
  name: simple-server-yml
  namespace: wg-ns
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app: simple-server
```
应用高可用主要有两点：
1) pod多实例部署，通过service实现负载均衡
2) pod之间相互反亲和，同时部署在不同的节点上
在添加反亲和之前可以看到有两个pod部署在同一个node1上
```bash
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-8476997874-jfrtm -o yaml | grep nodeName
  nodeName: node1
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-8476997874-vpr85 -o yaml | grep nodeName
  nodeName: node1
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-8476997874-wj94j -o yaml | grep nodeName
  nodeName: node2
```
添加pod反亲和性：
```yaml
affinity:
    podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
                operator: In
                values:
                - simple-server
        topologyKey: kubernetes.io/hostname
```

重新获取pod的信息如下：
```bash
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-644cfdb7d8-4d59d -o yaml  | grep nodeName
  nodeName: master
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-8476997874-vpr85 -o yaml  | grep nodeName
  nodeName: node1
wggg@node1:~$ kubectl get pod -n wg-ns simple-server-8476997874-wj94j -o yaml  | grep nodeName
  nodeName: node2
```
另外一个被驱逐的pod需要手动删除


__2. Ingress__


ingress的yaml如下
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gateway
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  # tls:
  #   - hosts:
  #       - wggg.com
  rules:
    - host: wggg.com
      http:
        paths:
          - path: "/"
            pathType: Prefix
            backend:
              service:
                name: simple-server-yml
                port:
                  number: 80

```
如何通过证书保证 httpServer 的通讯安全。
1. ingress实现https
2. 应用自身实现https通信




## 其他事宜

__现象一__：修复coreDNS的ready probe问题，以及master节点not ready的问题
> 通过查看kubelet的日志发现是主机改名了导致的链接失败问题

问题排查过程中尝试了以下一些方法：
- 方式一：~~直接kubectl edit节点的名称信息，但是metadata的内容是无法修改的~~
- 方式二：导出原有的节点yaml替换节点名称后再新建，使用kubeadm重建kubeconfig文件最后重启kubelet解决，参考如下的2


参考：
- [1.新建pod调试技巧](https://stackoverflow.com/questions/73297599/coredns-running-status-but-not-become-ready)
- [2.节点重置名字后，使用kubeadm进行修复](https://www.cnblogs.com/dudu/p/14286983.html)



__现象二__：namespace里面存在deployment资源但是在未删除的情况直接删除namespace
> namespace 一直处理terminating状态

解决手段：
- 清空namespace里的finalizer
- 查询未删除的资源并进行删除（但是deployment资源是看不到的，k8s也的确做了删除操作）

参考：
- [namespace-stuck-as-terminating-how-i-removed-it](https://stackoverflow.com/questions/52369247/namespace-stuck-as-terminating-how-i-removed-it)